{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import collections\n",
    "import random\n",
    "import json\n",
    "\n",
    "from data_modules.domainnet_dataset import ImageDataset\n",
    "from data_modules.domainnet_metadata import DOMAIN_NET_DOMAINS, DOMAIN_NET_CLASSES, DOMAIN_NET_DIVISIONS\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import v2 as T\n",
    "\n",
    "from yasin_utils.image import imagenet_normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "division_map = {}\n",
    "for k, v in DOMAIN_NET_DIVISIONS.items():\n",
    "    for i in v:\n",
    "        division_map[i] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root = '/data/domainnet_v1.0'\n",
    "# set_map = []\n",
    "# for domain in DOMAIN_NET_DOMAINS:\n",
    "#     try:\n",
    "#         labels = os.listdir(os.path.join(root, domain))\n",
    "#     except:\n",
    "#         raise Exception(f'{domain} directory not found.')\n",
    "#     for label in labels:\n",
    "#         for image in os.listdir(os.path.join(root, domain, label)):\n",
    "#             if label in division_map:\n",
    "#                 set_map.append(\n",
    "#                     dict(\n",
    "#                         img_path=os.path.join(root, domain, label, image),\n",
    "#                         label=division_map[label],\n",
    "#                         domain=domain\n",
    "#                         )\n",
    "#                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random.shuffle(set_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_set_map, test_set_map = set_map[:int(0.9*len(set_map))], set_map[int(0.9*len(set_map)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/yasin/repos/dispatch_smol/pretraining/data_modules/train_set_map_domainnet.json') as f:\n",
    "    train_set_map_ = json.load(f)\n",
    "with open('/home/yasin/repos/dispatch_smol/pretraining/data_modules/test_set_map_domainnet.json') as f:\n",
    "    test_set_map_ = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_map = []\n",
    "for sample in train_set_map_:\n",
    "    if sample['label'] in division_map:\n",
    "        new_sample = {**sample}\n",
    "        new_sample['label'] = division_map[sample['label']]\n",
    "        train_set_map.append(new_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_map = []\n",
    "for sample in test_set_map_:\n",
    "    if sample['label'] in division_map:\n",
    "        new_sample = {**sample}\n",
    "        new_sample['label'] = division_map[sample['label']]\n",
    "        test_set_map.append(new_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(241485, 26659)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set_map), len(test_set_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('tool', 'quickdraw'): 12600,\n",
       " ('tool', 'painting'): 4618,\n",
       " ('electricity', 'real'): 10563,\n",
       " ('cloth', 'infograph'): 3370,\n",
       " ('tool', 'real'): 11645,\n",
       " ('mammal', 'quickdraw'): 11309,\n",
       " ('cloth', 'real'): 10397,\n",
       " ('tool', 'clipart'): 3441,\n",
       " ('mammal', 'sketch'): 4644,\n",
       " ('electricity', 'painting'): 2669,\n",
       " ('furniture', 'painting'): 4472,\n",
       " ('cloth', 'painting'): 3192,\n",
       " ('furniture', 'sketch'): 6760,\n",
       " ('building', 'quickdraw'): 9479,\n",
       " ('furniture', 'real'): 15417,\n",
       " ('building', 'real'): 10874,\n",
       " ('mammal', 'clipart'): 3096,\n",
       " ('furniture', 'quickdraw'): 15745,\n",
       " ('mammal', 'infograph'): 3212,\n",
       " ('mammal', 'real'): 13996,\n",
       " ('electricity', 'infograph'): 3744,\n",
       " ('electricity', 'quickdraw'): 11284,\n",
       " ('cloth', 'sketch'): 4374,\n",
       " ('building', 'painting'): 4610,\n",
       " ('mammal', 'painting'): 8114,\n",
       " ('electricity', 'clipart'): 2803,\n",
       " ('cloth', 'clipart'): 3275,\n",
       " ('building', 'sketch'): 4241,\n",
       " ('furniture', 'clipart'): 5199,\n",
       " ('tool', 'sketch'): 4373,\n",
       " ('cloth', 'quickdraw'): 10285,\n",
       " ('building', 'clipart'): 2495,\n",
       " ('building', 'infograph'): 2840,\n",
       " ('furniture', 'infograph'): 5861,\n",
       " ('electricity', 'sketch'): 3683,\n",
       " ('tool', 'infograph'): 2805}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(collections.Counter([(d['label'], d['domain']) for d in train_set_map]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# balance_map = {\n",
    "#     'furniture': {'main_domain': 'clipart', 'target': 5_253},\n",
    "#     'cloth': {'main_domain': 'real', 'target': 10_377},\n",
    "#     'electricity': {'main_domain': 'infograph', 'target': 3_745},\n",
    "#     'building': {'main_domain': 'painting', 'target': 4_570},\n",
    "#     'mammal': {'main_domain': 'quickdraw', 'target': 11_202},\n",
    "#     'tool': {'main_domain': 'sketch', 'target': 4_401},\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "balance_map = {\n",
    "    'furniture': {'main_domain': 'clipart', 'target': 1_000},\n",
    "    'cloth': {'main_domain': 'real', 'target': 1_000},\n",
    "    'electricity': {'main_domain': 'infograph', 'target': 1_000},\n",
    "    'building': {'main_domain': 'painting', 'target': 1_000},\n",
    "    'mammal': {'main_domain': 'quickdraw', 'target': 1_000},\n",
    "    'tool': {'main_domain': 'sketch', 'target': 1_000},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# balance_map = {\n",
    "#     'furniture': {'main_domain': 'clipart', 'target': 1_000},\n",
    "#     'cloth': {'main_domain': 'real', 'target': 1_000},\n",
    "#     'electricity': {'main_domain': 'infograph', 'target': 1_000},\n",
    "#     'building': {'main_domain': 'painting', 'target': 1_000},\n",
    "#     'mammal': {'main_domain': 'quickdraw', 'target': 1_000},\n",
    "#     'tool': {'main_domain': 'sketch', 'target': 1_000},\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# balance_map = {\n",
    "#     'furniture': {'main_domain': 'clipart', 'target': 100},\n",
    "#     'cloth': {'main_domain': 'real', 'target': 100},\n",
    "#     'electricity': {'main_domain': 'infograph', 'target': 100},\n",
    "#     'building': {'main_domain': 'painting', 'target': 100},\n",
    "#     'mammal': {'main_domain': 'quickdraw', 'target': 100},\n",
    "#     'tool': {'main_domain': 'sketch', 'target': 100},\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune(set_map, balance_map):\n",
    "    pruned_set_map = []\n",
    "    \n",
    "    domains = set([i['domain'] for i in set_map])\n",
    "    counts = {\n",
    "        l: {\n",
    "            d: 0 for d in domains\n",
    "        } for l in balance_map\n",
    "    }\n",
    "    targets = {\n",
    "        l: {\n",
    "            d: balance_map[l]['target'] if d == balance_map[l]['main_domain'] else int(balance_map[l]['target']*0.01) for d in domains\n",
    "        } for l in balance_map\n",
    "    }\n",
    "\n",
    "    for i in set_map:\n",
    "        label, domain = i['label'], i['domain']\n",
    "\n",
    "        if counts[label][domain] < targets[label][domain]:\n",
    "            pruned_set_map.append(i)\n",
    "\n",
    "        counts[label][domain] += 1\n",
    "\n",
    "    return pruned_set_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_map = prune(train_set_map, balance_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('tool', 'quickdraw'): 10,\n",
       " ('tool', 'painting'): 10,\n",
       " ('electricity', 'real'): 10,\n",
       " ('cloth', 'infograph'): 10,\n",
       " ('tool', 'real'): 10,\n",
       " ('mammal', 'quickdraw'): 1000,\n",
       " ('cloth', 'real'): 1000,\n",
       " ('tool', 'clipart'): 10,\n",
       " ('mammal', 'sketch'): 10,\n",
       " ('electricity', 'painting'): 10,\n",
       " ('furniture', 'painting'): 10,\n",
       " ('cloth', 'painting'): 10,\n",
       " ('furniture', 'sketch'): 10,\n",
       " ('building', 'quickdraw'): 10,\n",
       " ('furniture', 'real'): 10,\n",
       " ('building', 'real'): 10,\n",
       " ('mammal', 'clipart'): 10,\n",
       " ('furniture', 'quickdraw'): 10,\n",
       " ('mammal', 'infograph'): 10,\n",
       " ('mammal', 'real'): 10,\n",
       " ('electricity', 'infograph'): 1000,\n",
       " ('electricity', 'quickdraw'): 10,\n",
       " ('cloth', 'sketch'): 10,\n",
       " ('building', 'painting'): 1000,\n",
       " ('mammal', 'painting'): 10,\n",
       " ('electricity', 'clipart'): 10,\n",
       " ('cloth', 'clipart'): 10,\n",
       " ('building', 'sketch'): 10,\n",
       " ('furniture', 'clipart'): 1000,\n",
       " ('tool', 'sketch'): 1000,\n",
       " ('cloth', 'quickdraw'): 10,\n",
       " ('building', 'clipart'): 10,\n",
       " ('building', 'infograph'): 10,\n",
       " ('furniture', 'infograph'): 10,\n",
       " ('electricity', 'sketch'): 10,\n",
       " ('tool', 'infograph'): 10}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(collections.Counter([(d['label'], d['domain']) for d in train_set_map]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnbalancedDomainNetDataset(ImageDataset):\n",
    "    def __init__(self, set_map, transform=None) -> None:\n",
    "\n",
    "        super().__init__(set_map, transform)\n",
    "        self.class_map = {\n",
    "            'furniture': 0,\n",
    "            'cloth': 1,\n",
    "            'electricity': 2,\n",
    "            'building': 3,\n",
    "            'mammal': 4,\n",
    "            'tool': 5,\n",
    "        }\n",
    "        self.domain_map = {\n",
    "            'clipart': 0,\n",
    "            'real': 1,\n",
    "            'infograph': 2,\n",
    "            'painting': 3,\n",
    "            'quickdraw': 4,\n",
    "            'sketch': 5,\n",
    "        }\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item = super().__getitem__(index)\n",
    "        item['label'] = self.class_map[item['label']]\n",
    "        item['domain'] = self.domain_map[item['domain']]\n",
    "\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yasin/miniforge3/lib/python3.10/site-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_transform = T.Compose([\n",
    "    T.RandomResizedCrop(128),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    # T.Resize((128,128)),\n",
    "    T.ToTensor(),\n",
    "    imagenet_normalize,\n",
    "])\n",
    "val_transform = T.Compose([\n",
    "    T.Resize(156),\n",
    "    T.CenterCrop(128),\n",
    "    # T.Resize((128,128)),\n",
    "    T.ToTensor(),\n",
    "    imagenet_normalize\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = UnbalancedDomainNetDataset(set_map=train_set_map, transform=train_transform)\n",
    "test_set = UnbalancedDomainNetDataset(set_map=test_set_map, transform=val_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, batch_size=256, shuffle=True, num_workers=8, pin_memory=True, persistent_workers=True, drop_last=True)\n",
    "val_loader = DataLoader(test_set, batch_size=256, shuffle=False, num_workers=8, pin_memory=True, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6300, 26659)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set), len(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import OrderedDict\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.models.resnet import resnet50, ResNet50_Weights\n",
    "import pytorch_lightning as L\n",
    "from torchmetrics import Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearProbe(L.LightningModule):\n",
    "    def __init__(self, backbone, emb_dim, num_classes, lr=1e-3) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.backbone: nn.Module = backbone\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.backbone.eval()\n",
    "\n",
    "        # self.linear_head: nn.Module = nn.Linear(emb_dim, num_classes)\n",
    "        self.linear_head: nn.Module = nn.Sequential(\n",
    "            nn.Linear(emb_dim, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.Linear(512, num_classes)\n",
    "        ) \n",
    "        \n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.lr = lr\n",
    "\n",
    "        self.test_accuracy = Accuracy(task='multiclass', num_classes=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.linear_head(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, bacth_idx):\n",
    "        X = batch['image']\n",
    "        t = batch['label']\n",
    "\n",
    "        y = self.forward(X)\n",
    "        loss = self.criterion(y, t)\n",
    "\n",
    "        print(loss.item())\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        X = batch['image']\n",
    "        t = batch['label']\n",
    "\n",
    "        y = self.forward(X)\n",
    "        acc = self.test_accuracy(y, t)\n",
    "\n",
    "        self.log('accuracy', acc, on_epoch=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.linear_head.parameters(), lr=self.lr)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 20)\n",
    "\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_backbone_from_ckpt(ckpt_path: str) -> torch.nn.Module:\n",
    "    state_dict = torch.load(ckpt_path)[\"state_dict\"]\n",
    "    state_dict = OrderedDict([\n",
    "        (\".\".join(name.split(\".\")[1:]), param) for name, param in state_dict.items() if name.startswith(\"backbone\")\n",
    "    ])\n",
    "\n",
    "    return state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline Model\n",
    "model_bl  = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "# weights_bl = get_backbone_from_ckpt(\"/home/yasin/Downloads/final_baseline.ckpt\")\n",
    "# model_bl.load_state_dict(weights_bl, strict=False)\n",
    "model_bl.fc = torch.nn.Identity()\n",
    "model_bl = model_bl.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MixStyle Model\n",
    "model_ms  = resnet50()\n",
    "weights_ms = get_backbone_from_ckpt(\"/home/yasin/Downloads/final_mixstyle.ckpt\")\n",
    "model_ms.load_state_dict(weights_ms, strict=False)\n",
    "model_ms.fc = torch.nn.Identity()\n",
    "model_ms = model_ms.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_module = LinearProbe(model_bl, emb_dim=2048, num_classes=6, lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixstyle_module = LinearProbe(model_ms, emb_dim=2048, num_classes=6, lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(\n",
    "    max_epochs=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type               | Params\n",
      "-----------------------------------------------------\n",
      "0 | backbone      | ResNet             | 23.5 M\n",
      "1 | linear_head   | Sequential         | 2.6 M \n",
      "2 | criterion     | CrossEntropyLoss   | 0     \n",
      "3 | test_accuracy | MulticlassAccuracy | 0     \n",
      "-----------------------------------------------------\n",
      "2.6 M     Trainable params\n",
      "23.5 M    Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.536   Total estimated model params size (MB)\n",
      "/home/yasin/miniforge3/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (24) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e40cc04696e45ec842e7a3f9114464f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8243143558502197\n",
      "1.476125717163086\n",
      "1.0754354000091553\n",
      "0.8993470668792725\n",
      "0.7957234382629395\n",
      "0.617152988910675\n",
      "0.7471188902854919\n",
      "0.7708296179771423\n",
      "0.9365196228027344\n",
      "0.6361665725708008\n",
      "0.6245900392532349\n",
      "0.6026021242141724\n",
      "0.6494312882423401\n",
      "0.45400765538215637\n",
      "0.5014895796775818\n",
      "0.667331337928772\n",
      "0.5263014435768127\n",
      "0.6133747696876526\n",
      "0.4492204785346985\n",
      "0.41073137521743774\n",
      "0.534043550491333\n",
      "0.615673303604126\n",
      "0.3925480842590332\n",
      "0.4419724941253662\n",
      "0.4501001536846161\n",
      "0.42576003074645996\n",
      "0.5080721974372864\n",
      "0.4747084975242615\n",
      "0.3985210061073303\n",
      "0.4329240620136261\n",
      "0.5123441815376282\n",
      "0.4530315399169922\n",
      "0.5329002141952515\n",
      "0.43316569924354553\n",
      "0.4037638306617737\n",
      "0.3545137047767639\n",
      "0.5766234993934631\n",
      "0.38909751176834106\n",
      "0.4978066682815552\n",
      "0.44544073939323425\n",
      "0.4476734399795532\n",
      "0.4516713321208954\n",
      "0.4189143776893616\n",
      "0.3120996952056885\n",
      "0.4625685214996338\n",
      "0.3722013831138611\n",
      "0.5079835653305054\n",
      "0.40774640440940857\n",
      "0.4167310297489166\n",
      "0.3454476296901703\n",
      "0.36670780181884766\n",
      "0.45628637075424194\n",
      "0.3647438585758209\n",
      "0.3385753035545349\n",
      "0.4638440012931824\n",
      "0.40897810459136963\n",
      "0.453701913356781\n",
      "0.42450734972953796\n",
      "0.4743093252182007\n",
      "0.3555890619754791\n",
      "0.37771135568618774\n",
      "0.3893599808216095\n",
      "0.32370778918266296\n",
      "0.33592626452445984\n",
      "0.38394561409950256\n",
      "0.37385058403015137\n",
      "0.39809921383857727\n",
      "0.37639904022216797\n",
      "0.4266067147254944\n",
      "0.34568703174591064\n",
      "0.46966010332107544\n",
      "0.3244892358779907\n",
      "0.28479620814323425\n",
      "0.24265289306640625\n",
      "0.4066702425479889\n",
      "0.31095924973487854\n",
      "0.3167891800403595\n",
      "0.3949466347694397\n",
      "0.3328491151332855\n",
      "0.3331414759159088\n",
      "0.2904183268547058\n",
      "0.2652285695075989\n",
      "0.34216031432151794\n",
      "0.3535906672477722\n",
      "0.3830745220184326\n",
      "0.3172403872013092\n",
      "0.3017781674861908\n",
      "0.37252023816108704\n",
      "0.29443833231925964\n",
      "0.40025925636291504\n",
      "0.25979581475257874\n",
      "0.4097141623497009\n",
      "0.3751983046531677\n",
      "0.2715049982070923\n",
      "0.33531418442726135\n",
      "0.4255600869655609\n",
      "0.28178244829177856\n",
      "0.2816561758518219\n",
      "0.27586162090301514\n",
      "0.31858357787132263\n",
      "0.26803746819496155\n",
      "0.31558406352996826\n",
      "0.24984849989414215\n",
      "0.28911304473876953\n",
      "0.3102063536643982\n",
      "0.34281834959983826\n",
      "0.415506511926651\n",
      "0.23833753168582916\n",
      "0.275108277797699\n",
      "0.32368308305740356\n",
      "0.31681734323501587\n",
      "0.34114140272140503\n",
      "0.2906881868839264\n",
      "0.33349379897117615\n",
      "0.2801273763179779\n",
      "0.3416091799736023\n",
      "0.2676236927509308\n",
      "0.2796955704689026\n",
      "0.3326077163219452\n",
      "0.32042351365089417\n",
      "0.23842543363571167\n",
      "0.2394578605890274\n",
      "0.2250385582447052\n",
      "0.25320205092430115\n",
      "0.2225068360567093\n",
      "0.22190910577774048\n",
      "0.3390749394893646\n",
      "0.24140675365924835\n",
      "0.29403573274612427\n",
      "0.33382347226142883\n",
      "0.3562569320201874\n",
      "0.3692860007286072\n",
      "0.3605327308177948\n",
      "0.2809084355831146\n",
      "0.3402910530567169\n",
      "0.3045901656150818\n",
      "0.2685961127281189\n",
      "0.25469720363616943\n",
      "0.24420547485351562\n",
      "0.3473997712135315\n",
      "0.3843708038330078\n",
      "0.2811046242713928\n",
      "0.2941121757030487\n",
      "0.29505404829978943\n",
      "0.27136677503585815\n",
      "0.28845715522766113\n",
      "0.26396483182907104\n",
      "0.28370094299316406\n",
      "0.2617018222808838\n",
      "0.22568106651306152\n",
      "0.2316981852054596\n",
      "0.29742586612701416\n",
      "0.1888779252767563\n",
      "0.2557167410850525\n",
      "0.20970571041107178\n",
      "0.28801897168159485\n",
      "0.1987941414117813\n",
      "0.2608775198459625\n",
      "0.3221132159233093\n",
      "0.20872415602207184\n",
      "0.3735548257827759\n",
      "0.26547518372535706\n",
      "0.22250328958034515\n",
      "0.23947128653526306\n",
      "0.2532021701335907\n",
      "0.22389967739582062\n",
      "0.2878955900669098\n",
      "0.2475591003894806\n",
      "0.2012604922056198\n",
      "0.22179411351680756\n",
      "0.1720239222049713\n",
      "0.18156911432743073\n",
      "0.24481403827667236\n",
      "0.16015374660491943\n",
      "0.30464285612106323\n",
      "0.2113245129585266\n",
      "0.18410418927669525\n",
      "0.2247598022222519\n",
      "0.22567197680473328\n",
      "0.2226249873638153\n",
      "0.3137039542198181\n",
      "0.2235533744096756\n",
      "0.2458137422800064\n",
      "0.2040305882692337\n",
      "0.19439469277858734\n",
      "0.299728125333786\n",
      "0.21301844716072083\n",
      "0.19624626636505127\n",
      "0.2822836935520172\n",
      "0.1967272311449051\n",
      "0.259132444858551\n",
      "0.2779640853404999\n",
      "0.1936865746974945\n",
      "0.19773249328136444\n",
      "0.17363093793392181\n",
      "0.21520286798477173\n",
      "0.1666688174009323\n",
      "0.20717135071754456\n",
      "0.17846225202083588\n",
      "0.2513200342655182\n",
      "0.2834177613258362\n",
      "0.19564612209796906\n",
      "0.1648649275302887\n",
      "0.25713011622428894\n",
      "0.1996244490146637\n",
      "0.20474165678024292\n",
      "0.17879338562488556\n",
      "0.24271471798419952\n",
      "0.18012025952339172\n",
      "0.2326873242855072\n",
      "0.1922612339258194\n",
      "0.1359712928533554\n",
      "0.20140713453292847\n",
      "0.24505266547203064\n",
      "0.16353978216648102\n",
      "0.2988218665122986\n",
      "0.142459437251091\n",
      "0.1083427369594574\n",
      "0.11772705614566803\n",
      "0.12195555865764618\n",
      "0.14942719042301178\n",
      "0.2557086646556854\n",
      "0.23636722564697266\n",
      "0.20615550875663757\n",
      "0.26702091097831726\n",
      "0.2508653998374939\n",
      "0.2621488869190216\n",
      "0.18563400208950043\n",
      "0.1505512148141861\n",
      "0.1655094474554062\n",
      "0.25053495168685913\n",
      "0.17444747686386108\n",
      "0.1732826828956604\n",
      "0.2276601940393448\n",
      "0.2072819471359253\n",
      "0.32803475856781006\n",
      "0.20480555295944214\n",
      "0.21124409139156342\n",
      "0.17917028069496155\n",
      "0.2208900898694992\n",
      "0.15627504885196686\n",
      "0.15220290422439575\n",
      "0.12521466612815857\n",
      "0.16196848452091217\n",
      "0.14765463769435883\n",
      "0.2505495250225067\n",
      "0.14052250981330872\n",
      "0.16429513692855835\n",
      "0.18956758081912994\n",
      "0.19001449644565582\n",
      "0.13127900660037994\n",
      "0.19212479889392853\n",
      "0.20073390007019043\n",
      "0.1894761174917221\n",
      "0.15182232856750488\n",
      "0.1915624439716339\n",
      "0.09189340472221375\n",
      "0.24272166192531586\n",
      "0.19373032450675964\n",
      "0.15129011869430542\n",
      "0.11326511949300766\n",
      "0.1296371966600418\n",
      "0.13367147743701935\n",
      "0.2328842729330063\n",
      "0.12683618068695068\n",
      "0.22320815920829773\n",
      "0.12047801911830902\n",
      "0.17320497334003448\n",
      "0.16014666855335236\n",
      "0.1292155236005783\n",
      "0.2157234251499176\n",
      "0.16706930100917816\n",
      "0.23544131219387054\n",
      "0.11561160534620285\n",
      "0.13946032524108887\n",
      "0.14935670793056488\n",
      "0.15295961499214172\n",
      "0.15013964474201202\n",
      "0.11584331095218658\n",
      "0.18873262405395508\n",
      "0.1669444590806961\n",
      "0.23066535592079163\n",
      "0.2030329555273056\n",
      "0.1718432903289795\n",
      "0.1543603539466858\n",
      "0.1363181173801422\n",
      "0.22710412740707397\n",
      "0.17602436244487762\n",
      "0.1803712099790573\n",
      "0.17304427921772003\n",
      "0.10740276426076889\n",
      "0.12908226251602173\n",
      "0.21296879649162292\n",
      "0.11668916046619415\n",
      "0.17135204374790192\n",
      "0.12062624841928482\n",
      "0.16001150012016296\n",
      "0.10494572669267654\n",
      "0.17318031191825867\n",
      "0.13036461174488068\n",
      "0.11422239989042282\n",
      "0.1047588512301445\n",
      "0.088019460439682\n",
      "0.18969081342220306\n",
      "0.1769990772008896\n",
      "0.1798873394727707\n",
      "0.16234178841114044\n",
      "0.1734815537929535\n",
      "0.16779956221580505\n",
      "0.16347715258598328\n",
      "0.09780456125736237\n",
      "0.12399677187204361\n",
      "0.1681208610534668\n",
      "0.18979784846305847\n",
      "0.14952582120895386\n",
      "0.16422529518604279\n",
      "0.11243702471256256\n",
      "0.1415780782699585\n",
      "0.1523425430059433\n",
      "0.1261640340089798\n",
      "0.15462526679039001\n",
      "0.2093280404806137\n",
      "0.10790804028511047\n",
      "0.14439581334590912\n",
      "0.125850647687912\n",
      "0.1897195279598236\n",
      "0.15050525963306427\n",
      "0.16026243567466736\n",
      "0.14842750132083893\n",
      "0.19779086112976074\n",
      "0.09285145252943039\n",
      "0.10584912449121475\n",
      "0.23443183302879333\n",
      "0.17619743943214417\n",
      "0.14530354738235474\n",
      "0.15193745493888855\n",
      "0.19350665807724\n",
      "0.1519526243209839\n",
      "0.1333370804786682\n",
      "0.14591556787490845\n",
      "0.1252381056547165\n",
      "0.18907327950000763\n",
      "0.1088385209441185\n",
      "0.16149091720581055\n",
      "0.14354340732097626\n",
      "0.1090526282787323\n",
      "0.10444442182779312\n",
      "0.12427197396755219\n",
      "0.17604166269302368\n",
      "0.08677094429731369\n",
      "0.17125405371189117\n",
      "0.17011503875255585\n",
      "0.1899702250957489\n",
      "0.1002538874745369\n",
      "0.12766817212104797\n",
      "0.12040954828262329\n",
      "0.14021921157836914\n",
      "0.09585389494895935\n",
      "0.1573674976825714\n",
      "0.0878738984465599\n",
      "0.09479842334985733\n",
      "0.15764552354812622\n",
      "0.14854766428470612\n",
      "0.11743824928998947\n",
      "0.15045404434204102\n",
      "0.15732420980930328\n",
      "0.10596878826618195\n",
      "0.14054538309574127\n",
      "0.21360552310943604\n",
      "0.15670931339263916\n",
      "0.11560813337564468\n",
      "0.15115505456924438\n",
      "0.18870849907398224\n",
      "0.1501455307006836\n",
      "0.1274944394826889\n",
      "0.14850476384162903\n",
      "0.11130031198263168\n",
      "0.2181302160024643\n",
      "0.15875343978405\n",
      "0.10500815510749817\n",
      "0.11863136291503906\n",
      "0.13646076619625092\n",
      "0.09872110933065414\n",
      "0.10301700234413147\n",
      "0.16208191215991974\n",
      "0.0778241828083992\n",
      "0.10991953313350677\n",
      "0.10788895934820175\n",
      "0.12155134230852127\n",
      "0.0881098061800003\n",
      "0.14395560324192047\n",
      "0.135904923081398\n",
      "0.11939481645822525\n",
      "0.12829722464084625\n",
      "0.18898752331733704\n",
      "0.09588740766048431\n",
      "0.1327212005853653\n",
      "0.14952857792377472\n",
      "0.19525521993637085\n",
      "0.12751297652721405\n",
      "0.09705660492181778\n",
      "0.22756783664226532\n",
      "0.19906935095787048\n",
      "0.17210866510868073\n",
      "0.08545321226119995\n",
      "0.14526240527629852\n",
      "0.1111106351017952\n",
      "0.10967712104320526\n",
      "0.1374417245388031\n",
      "0.12580659985542297\n",
      "0.12384918332099915\n",
      "0.06910808384418488\n",
      "0.13217028975486755\n",
      "0.09335602074861526\n",
      "0.1214536651968956\n",
      "0.10909353941679001\n",
      "0.1453782320022583\n",
      "0.09207557141780853\n",
      "0.10555310547351837\n",
      "0.12377671152353287\n",
      "0.12046413868665695\n",
      "0.09816331416368484\n",
      "0.10050005465745926\n",
      "0.15974129736423492\n",
      "0.1087706983089447\n",
      "0.1145922839641571\n",
      "0.12828516960144043\n",
      "0.12822207808494568\n",
      "0.149283766746521\n",
      "0.10593519359827042\n",
      "0.17516964673995972\n",
      "0.12632088363170624\n",
      "0.15928207337856293\n",
      "0.1948193907737732\n",
      "0.09107456356287003\n",
      "0.0777323916554451\n",
      "0.11860503256320953\n",
      "0.16525615751743317\n",
      "0.12725728750228882\n",
      "0.144220769405365\n",
      "0.09144540131092072\n",
      "0.07343880087137222\n",
      "0.1271209567785263\n",
      "0.08834252506494522\n",
      "0.1552756428718567\n",
      "0.11381391435861588\n",
      "0.0750722587108612\n",
      "0.11848638206720352\n",
      "0.16226470470428467\n",
      "0.09715525805950165\n",
      "0.11488254368305206\n",
      "0.18585070967674255\n",
      "0.15135280787944794\n",
      "0.06960699707269669\n",
      "0.13479138910770416\n",
      "0.132631316781044\n",
      "0.1010957881808281\n",
      "0.13063812255859375\n",
      "0.13717977702617645\n",
      "0.16213832795619965\n",
      "0.14867030084133148\n",
      "0.09786204248666763\n",
      "0.08008000254631042\n",
      "0.10063101351261139\n",
      "0.11325791478157043\n",
      "0.19788341224193573\n",
      "0.07952971756458282\n",
      "0.08854996412992477\n",
      "0.13223257660865784\n",
      "0.14323009550571442\n",
      "0.08108211308717728\n",
      "0.12421470135450363\n",
      "0.12983965873718262\n",
      "0.12927059829235077\n",
      "0.14831429719924927\n",
      "0.1265164464712143\n",
      "0.10445156693458557\n",
      "0.09013393521308899\n",
      "0.11518123745918274\n",
      "0.16850003600120544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(baseline_module, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/yasin/miniforge3/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:492: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53a966e7f5bf410cb7363deae6c66a19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yasin/miniforge3/lib/python3.10/site-packages/pytorch_lightning/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 256. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         accuracy          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9627278447151184     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        accuracy         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9627278447151184    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'accuracy': 0.9627278447151184}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(baseline_module, dataloaders=train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcf374d5c99045edb121693fe0619c1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yasin/miniforge3/lib/python3.10/site-packages/pytorch_lightning/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 35. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         accuracy          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.4554184377193451     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        accuracy         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.4554184377193451    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'accuracy': 0.4554184377193451}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(baseline_module, dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(\n",
    "    max_epochs=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type               | Params\n",
      "-----------------------------------------------------\n",
      "0 | backbone      | ResNet             | 23.5 M\n",
      "1 | linear_head   | Sequential         | 2.6 M \n",
      "2 | criterion     | CrossEntropyLoss   | 0     \n",
      "3 | test_accuracy | MulticlassAccuracy | 0     \n",
      "-----------------------------------------------------\n",
      "2.6 M     Trainable params\n",
      "23.5 M    Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.536   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "293ecb46326943edac338c76c4ff12ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8211008310317993\n",
      "1.396600604057312\n",
      "0.9942277669906616\n",
      "0.508196234703064\n",
      "0.48753172159194946\n",
      "0.7164696455001831\n",
      "0.4693939983844757\n",
      "0.7465087175369263\n",
      "0.628608226776123\n",
      "0.49176180362701416\n",
      "0.6297223567962646\n",
      "0.3639090359210968\n",
      "0.42854562401771545\n",
      "0.4240899384021759\n",
      "0.3046439588069916\n",
      "0.6265892386436462\n",
      "0.43210944533348083\n",
      "0.3792787790298462\n",
      "0.47830790281295776\n",
      "0.451437383890152\n",
      "0.48723673820495605\n",
      "0.4722122251987457\n",
      "0.4607580602169037\n",
      "0.31540998816490173\n",
      "0.28745168447494507\n",
      "0.2748413681983948\n",
      "0.4079006016254425\n",
      "0.27696913480758667\n",
      "0.29561448097229004\n",
      "0.30237677693367004\n",
      "0.27563002705574036\n",
      "0.3120799958705902\n",
      "0.20630724728107452\n",
      "0.292596697807312\n",
      "0.3027072548866272\n",
      "0.2741285264492035\n",
      "0.23733307421207428\n",
      "0.25165048241615295\n",
      "0.36302125453948975\n",
      "0.2211320847272873\n",
      "0.2894200384616852\n",
      "0.34269896149635315\n",
      "0.2811563014984131\n",
      "0.2058606594800949\n",
      "0.25917983055114746\n",
      "0.2532876133918762\n",
      "0.27679896354675293\n",
      "0.252267450094223\n",
      "0.2217949777841568\n",
      "0.24562206864356995\n",
      "0.18824906647205353\n",
      "0.18014027178287506\n",
      "0.17842721939086914\n",
      "0.17688792943954468\n",
      "0.19088827073574066\n",
      "0.2270365208387375\n",
      "0.13942232728004456\n",
      "0.13893067836761475\n",
      "0.21764442324638367\n",
      "0.15435682237148285\n",
      "0.3145510256290436\n",
      "0.19400542974472046\n",
      "0.25389334559440613\n",
      "0.20997366309165955\n",
      "0.1959005892276764\n",
      "0.1742321252822876\n",
      "0.21477434039115906\n",
      "0.21421287953853607\n",
      "0.19319012761116028\n",
      "0.21316012740135193\n",
      "0.19352884590625763\n",
      "0.23502469062805176\n",
      "0.15246039628982544\n",
      "0.22086715698242188\n",
      "0.2206512689590454\n",
      "0.18795748054981232\n",
      "0.18759001791477203\n",
      "0.22819726169109344\n",
      "0.15169920027256012\n",
      "0.16689619421958923\n",
      "0.19317637383937836\n",
      "0.13769091665744781\n",
      "0.24794252216815948\n",
      "0.14234407246112823\n",
      "0.1850803792476654\n",
      "0.1733531653881073\n",
      "0.1726190447807312\n",
      "0.20217329263687134\n",
      "0.196547269821167\n",
      "0.21278278529644012\n",
      "0.18844442069530487\n",
      "0.17359432578086853\n",
      "0.19713832437992096\n",
      "0.1884261518716812\n",
      "0.18334156274795532\n",
      "0.20367561280727386\n",
      "0.08470256626605988\n",
      "0.10736146569252014\n",
      "0.1195652112364769\n",
      "0.10350748896598816\n",
      "0.09170558303594589\n",
      "0.16489346325397491\n",
      "0.1644718199968338\n",
      "0.18669083714485168\n",
      "0.18518343567848206\n",
      "0.1652844250202179\n",
      "0.1556151658296585\n",
      "0.13347680866718292\n",
      "0.1043577715754509\n",
      "0.20611222088336945\n",
      "0.23654870688915253\n",
      "0.10604359954595566\n",
      "0.13430306315422058\n",
      "0.1240997388958931\n",
      "0.2064240723848343\n",
      "0.1812845915555954\n",
      "0.21708357334136963\n",
      "0.14028659462928772\n",
      "0.1390409618616104\n",
      "0.12011031806468964\n",
      "0.08663234859704971\n",
      "0.16687457263469696\n",
      "0.09342866390943527\n",
      "0.1338018923997879\n",
      "0.13784581422805786\n",
      "0.17435739934444427\n",
      "0.10556798428297043\n",
      "0.0853266716003418\n",
      "0.15445348620414734\n",
      "0.12510307133197784\n",
      "0.13956478238105774\n",
      "0.14357753098011017\n",
      "0.06401532888412476\n",
      "0.17006456851959229\n",
      "0.13755448162555695\n",
      "0.10945331305265427\n",
      "0.13907191157341003\n",
      "0.11121159791946411\n",
      "0.16847367584705353\n",
      "0.13988177478313446\n",
      "0.05269799754023552\n",
      "0.11222293972969055\n",
      "0.1496485322713852\n",
      "0.14038754999637604\n",
      "0.10336432605981827\n",
      "0.13375063240528107\n",
      "0.11516758054494858\n",
      "0.05765873193740845\n",
      "0.11874503642320633\n",
      "0.07294464111328125\n",
      "0.1217987984418869\n",
      "0.15000902116298676\n",
      "0.08794434368610382\n",
      "0.12649348378181458\n",
      "0.11554975807666779\n",
      "0.07389698177576065\n",
      "0.1285109519958496\n",
      "0.05632322281599045\n",
      "0.07518477737903595\n",
      "0.10997088998556137\n",
      "0.1342119425535202\n",
      "0.0629696324467659\n",
      "0.09435936063528061\n",
      "0.09197713434696198\n",
      "0.07674725353717804\n",
      "0.11986569315195084\n",
      "0.09475735574960709\n",
      "0.13296626508235931\n",
      "0.08323092013597488\n",
      "0.08508595824241638\n",
      "0.11533913761377335\n",
      "0.05414070934057236\n",
      "0.06929352879524231\n",
      "0.09338748455047607\n",
      "0.0511833056807518\n",
      "0.07969627529382706\n",
      "0.09365701675415039\n",
      "0.10752533376216888\n",
      "0.08844396471977234\n",
      "0.12429866194725037\n",
      "0.07106910645961761\n",
      "0.06525039672851562\n",
      "0.11795873194932938\n",
      "0.11975272744894028\n",
      "0.10519340634346008\n",
      "0.10799705237150192\n",
      "0.06885306537151337\n",
      "0.05161171033978462\n",
      "0.13802944123744965\n",
      "0.0769207552075386\n",
      "0.13253971934318542\n",
      "0.08645222336053848\n",
      "0.08081863075494766\n",
      "0.07964988052845001\n",
      "0.04069777950644493\n",
      "0.10336042195558548\n",
      "0.06738582253456116\n",
      "0.05530863255262375\n",
      "0.08749323338270187\n",
      "0.08412820845842361\n",
      "0.10194268822669983\n",
      "0.08737777918577194\n",
      "0.09767239540815353\n",
      "0.04742829501628876\n",
      "0.04404830560088158\n",
      "0.09852239489555359\n",
      "0.09154538810253143\n",
      "0.16104672849178314\n",
      "0.0580570250749588\n",
      "0.103848397731781\n",
      "0.07218137383460999\n",
      "0.07774727046489716\n",
      "0.09305061399936676\n",
      "0.08710354566574097\n",
      "0.12449668347835541\n",
      "0.056023918092250824\n",
      "0.07009288668632507\n",
      "0.054588135331869125\n",
      "0.044365476816892624\n",
      "0.11296513676643372\n",
      "0.039901647716760635\n",
      "0.04082515463232994\n",
      "0.06352274119853973\n",
      "0.09281229227781296\n",
      "0.055380016565322876\n",
      "0.05215990170836449\n",
      "0.05612744390964508\n",
      "0.06245146691799164\n",
      "0.09692154824733734\n",
      "0.11137237399816513\n",
      "0.0786980390548706\n",
      "0.04697277396917343\n",
      "0.06376887112855911\n",
      "0.0950595960021019\n",
      "0.04179719462990761\n",
      "0.10818441212177277\n",
      "0.04319063574075699\n",
      "0.11101019382476807\n",
      "0.11442489922046661\n",
      "0.061753351241350174\n",
      "0.07327120006084442\n",
      "0.09484756737947464\n",
      "0.09251310676336288\n",
      "0.10226631909608841\n",
      "0.1119050532579422\n",
      "0.05369005352258682\n",
      "0.11454395949840546\n",
      "0.09420564025640488\n",
      "0.05026039481163025\n",
      "0.058965157717466354\n",
      "0.04004790633916855\n",
      "0.08305317908525467\n",
      "0.02893541194498539\n",
      "0.13357585668563843\n",
      "0.11443836241960526\n",
      "0.10769051313400269\n",
      "0.052600134164094925\n",
      "0.045934826135635376\n",
      "0.07881193608045578\n",
      "0.07554991543292999\n",
      "0.04391173645853996\n",
      "0.03624837473034859\n",
      "0.051272403448820114\n",
      "0.12844355404376984\n",
      "0.06648671627044678\n",
      "0.09349481016397476\n",
      "0.07701504975557327\n",
      "0.05669555813074112\n",
      "0.0371260866522789\n",
      "0.044814787805080414\n",
      "0.06771525740623474\n",
      "0.07321225106716156\n",
      "0.12449252605438232\n",
      "0.06473952531814575\n",
      "0.13068746030330658\n",
      "0.07201991230249405\n",
      "0.06715812534093857\n",
      "0.06075913459062576\n",
      "0.06588194519281387\n",
      "0.1080055832862854\n",
      "0.11594831198453903\n",
      "0.05919615551829338\n",
      "0.06619512289762497\n",
      "0.04202570393681526\n",
      "0.10978752374649048\n",
      "0.04540107771754265\n",
      "0.05615758150815964\n",
      "0.04431123659014702\n",
      "0.08288007974624634\n",
      "0.09038046002388\n",
      "0.06742702424526215\n",
      "0.07612434774637222\n",
      "0.06023583188652992\n",
      "0.050698887556791306\n",
      "0.061851706355810165\n",
      "0.07679642736911774\n",
      "0.03611716255545616\n",
      "0.05801993981003761\n",
      "0.030061183497309685\n",
      "0.04835778474807739\n",
      "0.07433459162712097\n",
      "0.12083664536476135\n",
      "0.054989177733659744\n",
      "0.07563896477222443\n",
      "0.05504225194454193\n",
      "0.03514203056693077\n",
      "0.08680932223796844\n",
      "0.09157169610261917\n",
      "0.043418049812316895\n",
      "0.04941940680146217\n",
      "0.062014900147914886\n",
      "0.060684144496917725\n",
      "0.056073855608701706\n",
      "0.09657797962427139\n",
      "0.05656201019883156\n",
      "0.10530497133731842\n",
      "0.05026322975754738\n",
      "0.04001159965991974\n",
      "0.05359769985079765\n",
      "0.041851453483104706\n",
      "0.022996820509433746\n",
      "0.07492272555828094\n",
      "0.15517422556877136\n",
      "0.02501079812645912\n",
      "0.06874080002307892\n",
      "0.06856220960617065\n",
      "0.0804940015077591\n",
      "0.02656540274620056\n",
      "0.044647566974163055\n",
      "0.07676048576831818\n",
      "0.09557648003101349\n",
      "0.0682411640882492\n",
      "0.04801252484321594\n",
      "0.055929120630025864\n",
      "0.0643644779920578\n",
      "0.03115883469581604\n",
      "0.10037915408611298\n",
      "0.03149713948369026\n",
      "0.056614045053720474\n",
      "0.022672826424241066\n",
      "0.04586201161146164\n",
      "0.03574918583035469\n",
      "0.04765641316771507\n",
      "0.0669550970196724\n",
      "0.07230912148952484\n",
      "0.08397012203931808\n",
      "0.017647435888648033\n",
      "0.05377718433737755\n",
      "0.051550425589084625\n",
      "0.07931380718946457\n",
      "0.04073427617549896\n",
      "0.021788036450743675\n",
      "0.046614617109298706\n",
      "0.05245646834373474\n",
      "0.041112709790468216\n",
      "0.05857729911804199\n",
      "0.07469038665294647\n",
      "0.06279047578573227\n",
      "0.04023924842476845\n",
      "0.045798152685165405\n",
      "0.04194023832678795\n",
      "0.029409416019916534\n",
      "0.06165728345513344\n",
      "0.06525255739688873\n",
      "0.045662056654691696\n",
      "0.07773945480585098\n",
      "0.05798127129673958\n",
      "0.032738812267780304\n",
      "0.04808365926146507\n",
      "0.06599247455596924\n",
      "0.0981905460357666\n",
      "0.06928358972072601\n",
      "0.04189284145832062\n",
      "0.03921426832675934\n",
      "0.04452964663505554\n",
      "0.036434851586818695\n",
      "0.07190342992544174\n",
      "0.03738629072904587\n",
      "0.08339185267686844\n",
      "0.025547904893755913\n",
      "0.03634690120816231\n",
      "0.06982783228158951\n",
      "0.05266158655285835\n",
      "0.06179685518145561\n",
      "0.029970470815896988\n",
      "0.04422623664140701\n",
      "0.0854034349322319\n",
      "0.039627473801374435\n",
      "0.06964999437332153\n",
      "0.052340779453516006\n",
      "0.05035847797989845\n",
      "0.010598532855510712\n",
      "0.05757124349474907\n",
      "0.02800917997956276\n",
      "0.07143335789442062\n",
      "0.04353903979063034\n",
      "0.022551750764250755\n",
      "0.03287089243531227\n",
      "0.040011607110500336\n",
      "0.06739770621061325\n",
      "0.0579143725335598\n",
      "0.053067535161972046\n",
      "0.035687629133462906\n",
      "0.03264622390270233\n",
      "0.05754416435956955\n",
      "0.037456050515174866\n",
      "0.10791251063346863\n",
      "0.06691146641969681\n",
      "0.030642155557870865\n",
      "0.0414106547832489\n",
      "0.04184072092175484\n",
      "0.08884606510400772\n",
      "0.04745785519480705\n",
      "0.02354181930422783\n",
      "0.04943292960524559\n",
      "0.06638333946466446\n",
      "0.05489806458353996\n",
      "0.04708855599164963\n",
      "0.03764402121305466\n",
      "0.05640585348010063\n",
      "0.0656592845916748\n",
      "0.06392931938171387\n",
      "0.024460235610604286\n",
      "0.05538008734583855\n",
      "0.02516791597008705\n",
      "0.01986677199602127\n",
      "0.025510987266898155\n",
      "0.039490461349487305\n",
      "0.07240407168865204\n",
      "0.04781912639737129\n",
      "0.0578342080116272\n",
      "0.04280046001076698\n",
      "0.033574920147657394\n",
      "0.02625800296664238\n",
      "0.018859265372157097\n",
      "0.10517042130231857\n",
      "0.08318917453289032\n",
      "0.08788279443979263\n",
      "0.03680558502674103\n",
      "0.04832925647497177\n",
      "0.030226320028305054\n",
      "0.018469320610165596\n",
      "0.027674494311213493\n",
      "0.06812716275453568\n",
      "0.02389223501086235\n",
      "0.04028073698282242\n",
      "0.07546520233154297\n",
      "0.024400416761636734\n",
      "0.03676588460803032\n",
      "0.028498152270913124\n",
      "0.035850901156663895\n",
      "0.08807963877916336\n",
      "0.07359551638364792\n",
      "0.06462565809488297\n",
      "0.054429955780506134\n",
      "0.07193261384963989\n",
      "0.0739990770816803\n",
      "0.04987237975001335\n",
      "0.074375681579113\n",
      "0.047133225947618484\n",
      "0.08863086253404617\n",
      "0.054578348994255066\n",
      "0.01757298968732357\n",
      "0.037303660064935684\n",
      "0.03904007747769356\n",
      "0.04360279068350792\n",
      "0.011961572803556919\n",
      "0.07597697526216507\n",
      "0.07494238018989563\n",
      "0.07324866950511932\n",
      "0.04527227580547333\n",
      "0.04150199145078659\n",
      "0.07129665464162827\n",
      "0.027731066569685936\n",
      "0.03859655186533928\n",
      "0.0476142019033432\n",
      "0.024039730429649353\n",
      "0.029513662680983543\n",
      "0.03238842263817787\n",
      "0.08397074788808823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(mixstyle_module, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1dc1559a4e34805a73c27a0d80d4d29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         accuracy          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9830729365348816     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        accuracy         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9830729365348816    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'accuracy': 0.9830729365348816}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(mixstyle_module, dataloaders=train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a3db0b84d694985b621564070d29e6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         accuracy          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.5080085396766663     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        accuracy         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5080085396766663    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'accuracy': 0.5080085396766663}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(mixstyle_module, dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| # Samples | Baseline | MixStyle |\n",
    "| --------- | -------- | -------- |\n",
    "| All       | 0.629    | 0.642    |\n",
    "| 1_000     | 0.569    | 0.576    |\n",
    "| 500       | 0.520    | 0.538    |\n",
    "| 100       | 0.451    | 0.463    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
